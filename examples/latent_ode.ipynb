{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--adjoint', type=eval, default=False)\n",
    "parser.add_argument('--visualize', type=eval, default=False)\n",
    "parser.add_argument('--niters', type=int, default=2000)\n",
    "parser.add_argument('--lr', type=float, default=0.01)\n",
    "parser.add_argument('--gpu', type=int, default=0)\n",
    "parser.add_argument('--train_dir', type=str, default=None)\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.adjoint:\n",
    "    from torchdiffeq import odeint_adjoint as odeint\n",
    "else:\n",
    "    from torchdiffeq import odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spiral2d(nspiral=1000,\n",
    "                      ntotal=500,\n",
    "                      nsample=100,\n",
    "                      start=0.,\n",
    "                      stop=1,  # approximately equal to 6pi\n",
    "                      noise_std=.1,\n",
    "                      a=0.,\n",
    "                      b=1.,\n",
    "                      savefig=True):\n",
    "    \"\"\"Parametric formula for 2d spiral is `r = a + b * theta`.\n",
    "\n",
    "    Args:\n",
    "      nspiral: number of spirals, i.e. batch dimension\n",
    "      ntotal: total number of datapoints per spiral\n",
    "      nsample: number of sampled datapoints for model fitting per spiral\n",
    "      start: spiral starting theta value\n",
    "      stop: spiral ending theta value\n",
    "      noise_std: observation noise standard deviation\n",
    "      a, b: parameters of the Archimedean spiral\n",
    "      savefig: plot the ground truth for sanity check\n",
    "\n",
    "    Returns: \n",
    "      Tuple where first element is true trajectory of size (nspiral, ntotal, 2),\n",
    "      second element is noisy observations of size (nspiral, nsample, 2),\n",
    "      third element is timestamps of size (ntotal,),\n",
    "      and fourth element is timestamps of size (nsample,)\n",
    "    \"\"\"\n",
    "\n",
    "    # add 1 all timestamps to avoid division by 0\n",
    "    orig_ts = np.linspace(start, stop, num=ntotal)\n",
    "    samp_ts = orig_ts[:nsample]\n",
    "\n",
    "    # generate clock-wise and counter clock-wise spirals in observation space\n",
    "    # with two sets of time-invariant latent dynamics\n",
    "    zs_cw = stop + 1. - orig_ts\n",
    "    rs_cw = a + b * 50. / zs_cw\n",
    "    xs, ys = rs_cw * np.cos(zs_cw) - 5., rs_cw * np.sin(zs_cw)\n",
    "    orig_traj_cw = np.stack((xs, ys), axis=1)\n",
    "\n",
    "    zs_cc = orig_ts\n",
    "    rw_cc = a + b * zs_cc\n",
    "    xs, ys = rw_cc * np.cos(zs_cc) + 5., rw_cc * np.sin(zs_cc)\n",
    "    orig_traj_cc = np.stack((xs, ys), axis=1)\n",
    "\n",
    "    if savefig:\n",
    "        plt.figure()\n",
    "        plt.plot(orig_traj_cw[:, 0], orig_traj_cw[:, 1], label='clock')\n",
    "        plt.plot(orig_traj_cc[:, 0], orig_traj_cc[:, 1], label='counter clock')\n",
    "        plt.legend()\n",
    "        plt.savefig('./ground_truth.png', dpi=500)\n",
    "        print('Saved ground truth spiral at {}'.format('./ground_truth.png'))\n",
    "\n",
    "    # sample starting timestamps\n",
    "    orig_trajs = []\n",
    "    samp_trajs = []\n",
    "    for _ in range(nspiral):\n",
    "        # don't sample t0 very near the start or the end\n",
    "        t0_idx = npr.multinomial(\n",
    "            1, [1. / (ntotal - 2. * nsample)] * (ntotal - int(2 * nsample)))\n",
    "        t0_idx = np.argmax(t0_idx) + nsample\n",
    "\n",
    "        cc = bool(npr.rand() > .5)  # uniformly select rotation\n",
    "        orig_traj = orig_traj_cc if cc else orig_traj_cw\n",
    "        orig_trajs.append(orig_traj)\n",
    "\n",
    "        samp_traj = orig_traj[t0_idx:t0_idx + nsample, :].copy()\n",
    "        samp_traj += npr.randn(*samp_traj.shape) * noise_std\n",
    "        samp_trajs.append(samp_traj)\n",
    "\n",
    "    # batching for sample trajectories is good for RNN; batching for original\n",
    "    # trajectories only for ease of indexing\n",
    "    orig_trajs = np.stack(orig_trajs, axis=0)\n",
    "    samp_trajs = np.stack(samp_trajs, axis=0)\n",
    "\n",
    "    return orig_trajs, samp_trajs, orig_ts, samp_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentODEfunc(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim=4, nhidden=20):\n",
    "        super(LatentODEfunc, self).__init__()\n",
    "        self.elu = nn.ELU(inplace=True)\n",
    "        self.fc1 = nn.Linear(latent_dim, nhidden)\n",
    "        self.fc2 = nn.Linear(nhidden, nhidden)\n",
    "        self.fc3 = nn.Linear(nhidden, latent_dim)\n",
    "        self.nfe = 0\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        self.nfe += 1\n",
    "        out = self.fc1(x)\n",
    "        out = self.elu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.elu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecognitionRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim=4, obs_dim=2, nhidden=25, nbatch=1):\n",
    "        super(RecognitionRNN, self).__init__()\n",
    "        self.nhidden = nhidden\n",
    "        self.nbatch = nbatch\n",
    "        self.i2h = nn.Linear(obs_dim + nhidden, nhidden)\n",
    "        self.h2o = nn.Linear(nhidden, latent_dim * 2)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        combined = torch.cat((x, h), dim=1)\n",
    "        h = torch.tanh(self.i2h(combined))\n",
    "        out = self.h2o(h)\n",
    "        return out, h\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.nbatch, self.nhidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim=4, obs_dim=2, nhidden=20):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc1 = nn.Linear(latent_dim, nhidden)\n",
    "        self.fc2 = nn.Linear(nhidden, obs_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.fc1(z)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningAverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, momentum=0.99):\n",
    "        self.momentum = momentum\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = None\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        if self.val is None:\n",
    "            self.avg = val\n",
    "        else:\n",
    "            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\n",
    "        self.val = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_pdf(x, mean, logvar):\n",
    "    const = torch.from_numpy(np.array([2. * np.pi])).float().to(x.device)\n",
    "    const = torch.log(const)\n",
    "    return -.5 * (const + logvar + (x - mean) ** 2. / torch.exp(logvar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_kl(mu1, lv1, mu2, lv2):\n",
    "    v1 = torch.exp(lv1)\n",
    "    v2 = torch.exp(lv2)\n",
    "    lstd1 = lv1 / 2.\n",
    "    lstd2 = lv2 / 2.\n",
    "\n",
    "    kl = lstd2 - lstd1 + ((v1 + (mu1 - mu2) ** 2.) / (2. * v2)) - .5\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    latent_dim = 4\n",
    "    nhidden = 20\n",
    "    rnn_nhidden = 25\n",
    "    obs_dim = 2\n",
    "    nspiral = 1000\n",
    "    start = 0.\n",
    "    stop = 6 * np.pi\n",
    "    noise_std = .3\n",
    "    a = 0.\n",
    "    b = .3\n",
    "    ntotal = 1000\n",
    "    nsample = 100\n",
    "    device = torch.device('cuda:' + str(args.gpu)\n",
    "                          if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # generate toy spiral data\n",
    "    orig_trajs, samp_trajs, orig_ts, samp_ts = generate_spiral2d(\n",
    "        nspiral=nspiral,\n",
    "        start=start,\n",
    "        stop=stop,\n",
    "        noise_std=noise_std,\n",
    "        a=a, b=b\n",
    "    )\n",
    "    orig_trajs = torch.from_numpy(orig_trajs).float().to(device)\n",
    "    samp_trajs = torch.from_numpy(samp_trajs).float().to(device)\n",
    "    samp_ts = torch.from_numpy(samp_ts).float().to(device)\n",
    "\n",
    "    # model\n",
    "    func = LatentODEfunc(latent_dim, nhidden).to(device)\n",
    "    rec = RecognitionRNN(latent_dim, obs_dim, rnn_nhidden, nspiral).to(device)\n",
    "    dec = Decoder(latent_dim, obs_dim, nhidden).to(device)\n",
    "    params = (list(func.parameters()) + list(dec.parameters()) + list(rec.parameters()))\n",
    "    optimizer = optim.Adam(params, lr=args.lr)\n",
    "    loss_meter = RunningAverageMeter()\n",
    "\n",
    "    if args.train_dir is not None:\n",
    "        if not os.path.exists(args.train_dir):\n",
    "            os.makedirs(args.train_dir)\n",
    "        ckpt_path = os.path.join(args.train_dir, 'ckpt.pth')\n",
    "        if os.path.exists(ckpt_path):\n",
    "            checkpoint = torch.load(ckpt_path)\n",
    "            func.load_state_dict(checkpoint['func_state_dict'])\n",
    "            rec.load_state_dict(checkpoint['rec_state_dict'])\n",
    "            dec.load_state_dict(checkpoint['dec_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            orig_trajs = checkpoint['orig_trajs']\n",
    "            samp_trajs = checkpoint['samp_trajs']\n",
    "            orig_ts = checkpoint['orig_ts']\n",
    "            samp_ts = checkpoint['samp_ts']\n",
    "            print('Loaded ckpt from {}'.format(ckpt_path))\n",
    "\n",
    "    try:\n",
    "        for itr in range(1, args.niters + 1):\n",
    "            optimizer.zero_grad()\n",
    "            # backward in time to infer q(z_0)\n",
    "            h = rec.initHidden().to(device)\n",
    "            for t in reversed(range(samp_trajs.size(1))):\n",
    "                obs = samp_trajs[:, t, :]\n",
    "                out, h = rec.forward(obs, h)\n",
    "            qz0_mean, qz0_logvar = out[:, :latent_dim], out[:, latent_dim:]\n",
    "            epsilon = torch.randn(qz0_mean.size()).to(device)\n",
    "            z0 = epsilon * torch.exp(.5 * qz0_logvar) + qz0_mean\n",
    "\n",
    "            # forward in time and solve ode for reconstructions\n",
    "            pred_z = odeint(func, z0, samp_ts).permute(1, 0, 2)\n",
    "            pred_x = dec(pred_z)\n",
    "\n",
    "            # compute loss\n",
    "            noise_std_ = torch.zeros(pred_x.size()).to(device) + noise_std\n",
    "            noise_logvar = 2. * torch.log(noise_std_).to(device)\n",
    "            logpx = log_normal_pdf(\n",
    "                samp_trajs, pred_x, noise_logvar).sum(-1).sum(-1)\n",
    "            pz0_mean = pz0_logvar = torch.zeros(z0.size()).to(device)\n",
    "            analytic_kl = normal_kl(qz0_mean, qz0_logvar,\n",
    "                                    pz0_mean, pz0_logvar).sum(-1)\n",
    "            loss = torch.mean(-logpx + analytic_kl, dim=0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_meter.update(loss.item())\n",
    "\n",
    "            print('Iter: {}, running avg elbo: {:.4f}'.format(itr, -loss_meter.avg))\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        if args.train_dir is not None:\n",
    "            ckpt_path = os.path.join(args.train_dir, 'ckpt.pth')\n",
    "            torch.save({\n",
    "                'func_state_dict': func.state_dict(),\n",
    "                'rec_state_dict': rec.state_dict(),\n",
    "                'dec_state_dict': dec.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'orig_trajs': orig_trajs,\n",
    "                'samp_trajs': samp_trajs,\n",
    "                'orig_ts': orig_ts,\n",
    "                'samp_ts': samp_ts,\n",
    "            }, ckpt_path)\n",
    "            print('Stored ckpt at {}'.format(ckpt_path))\n",
    "    print('Training complete after {} iters.'.format(itr))\n",
    "\n",
    "    if args.visualize:\n",
    "        with torch.no_grad():\n",
    "            # sample from trajectorys' approx. posterior\n",
    "            h = rec.initHidden().to(device)\n",
    "            for t in reversed(range(samp_trajs.size(1))):\n",
    "                obs = samp_trajs[:, t, :]\n",
    "                out, h = rec.forward(obs, h)\n",
    "            qz0_mean, qz0_logvar = out[:, :latent_dim], out[:, latent_dim:]\n",
    "            epsilon = torch.randn(qz0_mean.size()).to(device)\n",
    "            z0 = epsilon * torch.exp(.5 * qz0_logvar) + qz0_mean\n",
    "            orig_ts = torch.from_numpy(orig_ts).float().to(device)\n",
    "\n",
    "            # take first trajectory for visualization\n",
    "            z0 = z0[0]\n",
    "\n",
    "            ts_pos = np.linspace(0., 2. * np.pi, num=2000)\n",
    "            ts_neg = np.linspace(-np.pi, 0., num=2000)[::-1].copy()\n",
    "            ts_pos = torch.from_numpy(ts_pos).float().to(device)\n",
    "            ts_neg = torch.from_numpy(ts_neg).float().to(device)\n",
    "\n",
    "            zs_pos = odeint(func, z0, ts_pos)\n",
    "            zs_neg = odeint(func, z0, ts_neg)\n",
    "\n",
    "            xs_pos = dec(zs_pos)\n",
    "            xs_neg = torch.flip(dec(zs_neg), dims=[0])\n",
    "\n",
    "        xs_pos = xs_pos.cpu().numpy()\n",
    "        xs_neg = xs_neg.cpu().numpy()\n",
    "        orig_traj = orig_trajs[0].cpu().numpy()\n",
    "        samp_traj = samp_trajs[0].cpu().numpy()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(orig_traj[:, 0], orig_traj[:, 1],\n",
    "                 'g', label='true trajectory')\n",
    "        plt.plot(xs_pos[:, 0], xs_pos[:, 1], 'r',\n",
    "                 label='learned trajectory (t>0)')\n",
    "        plt.plot(xs_neg[:, 0], xs_neg[:, 1], 'c',\n",
    "                 label='learned trajectory (t<0)')\n",
    "        plt.scatter(samp_traj[:, 0], samp_traj[\n",
    "                    :, 1], label='sampled data', s=3)\n",
    "        plt.legend()\n",
    "        plt.savefig('./vis.png', dpi=500)\n",
    "        print('Saved visualization figure at {}'.format('./vis.png'))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
